{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f016ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session running in VS Code!\n",
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VSCodeTest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session running in VS Code!\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2337f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Connect to your local Ollama server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"  # Dummy key required by SDK\n",
    ")\n",
    "\n",
    "def ask_local_gpt(prompt: str, model=\"gpt-oss:20b\", system_message=None, render_markdown=True):\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    raw_output = response.choices[0].message.content\n",
    "\n",
    "    # Normalize spacing but preserve formatting\n",
    "    clean_output = unicodedata.normalize(\"NFKC\", raw_output).replace(\"\\u202f\", \" \")\n",
    "\n",
    "    if render_markdown:\n",
    "        display(Markdown(clean_output))\n",
    "    return clean_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24003588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Connect to your local Ollama server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"  # Dummy key required by SDK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30b22b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_log = []\n",
    "chat_memory = []\n",
    "\n",
    "def ask_local_gpt(\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-oss:20b\",\n",
    "    system_message: Optional[str] = None,\n",
    "    render_markdown: bool = True,\n",
    "    verbose: bool = False,\n",
    "    return_raw: bool = False,\n",
    "    reset_chat: bool = False,\n",
    "    show_history: bool = False,\n",
    "    stream_response: bool = True,\n",
    "    reasoning_mode: bool = False\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Query a local GPT model via Ollama, with support for step-by-step reasoning (Scratchpad).\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user‚Äôs input/question.\n",
    "        model (str): Model name (default 'gpt-oss:20b').\n",
    "        system_message (str, optional): Optional system prompt.\n",
    "        render_markdown (bool): If True, pretty-print answer as markdown in notebook.\n",
    "        verbose (bool): If True, print extra info.\n",
    "        return_raw (bool): If True, return the full unprocessed model response.\n",
    "        reset_chat (bool): If True, reset the conversation history.\n",
    "        show_history (bool): If True, print the message history after each call.\n",
    "        stream_response (bool): If True, stream the response token by token.\n",
    "        reasoning_mode (bool): If True, injects scratchpad (\"thinking out loud\") prompt.\n",
    "\n",
    "    Returns:\n",
    "        str or None: Clean final answer or full output, depending on flags.\n",
    "    \"\"\"\n",
    "    global chat_memory, conversation_log\n",
    "\n",
    "    try:\n",
    "        if reset_chat:\n",
    "            chat_memory = []\n",
    "\n",
    "        # System prompt for reasoning mode\n",
    "        default_reasoning_prompt = (\n",
    "            \"You are a helpful assistant that always reasons step by step before giving an answer. \"\n",
    "            \"First, think through the problem, then provide a clear final answer.\"\n",
    "        )\n",
    "\n",
    "        # Add a system message only if not present\n",
    "        if not any(m.get(\"role\") == \"system\" for m in chat_memory):\n",
    "            chat_memory.insert(0, {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message or (default_reasoning_prompt if reasoning_mode else \"\")\n",
    "            })\n",
    "\n",
    "        # Prefix the prompt with reasoning instructions if enabled\n",
    "        if reasoning_mode:\n",
    "            full_prompt = (\n",
    "                f\"### Scratchpad:\\n\"\n",
    "                f\"The user asked: \\\"{prompt.strip()}\\\"\\n\"\n",
    "                f\"Think step-by-step and reason before answering.\\n\\n\"\n",
    "                f\"### Final Answer:\\n\"\n",
    "            )\n",
    "        else:\n",
    "            full_prompt = prompt\n",
    "\n",
    "        chat_memory.append({\"role\": \"user\", \"content\": full_prompt})\n",
    "        start = time.time()\n",
    "\n",
    "        # Get the response (stream or not)\n",
    "        if stream_response:\n",
    "            print(\"ü§î Thinking...\\n\")\n",
    "            stream = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=chat_memory,\n",
    "                stream=True\n",
    "            )\n",
    "            tokens = []\n",
    "            for chunk in stream:\n",
    "                delta = chunk.choices[0].delta.content or \"\"\n",
    "                print(delta, end=\"\", flush=True)\n",
    "                tokens.append(delta)\n",
    "            print()\n",
    "            assistant_reply = \"\".join(tokens)\n",
    "        else:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=chat_memory\n",
    "            )\n",
    "            assistant_reply = response.choices[0].message.content\n",
    "\n",
    "        chat_memory.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "        # ---- Parse the thoughts/scratchpad, final answer, and raw ----\n",
    "        # Default values\n",
    "        thoughts = \"\"\n",
    "        final_answer = assistant_reply.strip()\n",
    "\n",
    "        # Extract \"Scratchpad\" (step-by-step reasoning) and \"Final Answer\"\n",
    "        scratchpad_match = re.search(\n",
    "            r\"### Scratchpad:\\s*(.*?)### Final Answer:\",\n",
    "            assistant_reply,\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        final_answer_match = re.search(\n",
    "            r\"### Final Answer:\\s*([\\s\\S]*?)(?:$|\\n#|\\n\\n)\",\n",
    "            assistant_reply,\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        if scratchpad_match:\n",
    "            # Remove markdown, extra whitespace, etc. from scratchpad\n",
    "            thoughts = scratchpad_match.group(1).strip()\n",
    "        if final_answer_match:\n",
    "            # Strip any trailing markdown/whitespace for the cleanest final answer\n",
    "            final_answer = final_answer_match.group(1).strip()\n",
    "\n",
    "        # Save everything to log for later analysis/export\n",
    "        conversation_log.append({\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"prompt\": prompt,\n",
    "            \"thoughts\": thoughts,\n",
    "            \"answer\": final_answer,\n",
    "            \"raw\": assistant_reply\n",
    "        })\n",
    "\n",
    "        # Choose output behavior based on flags\n",
    "        if return_raw:\n",
    "            return assistant_reply\n",
    "        elif render_markdown:\n",
    "            display(Markdown(final_answer))\n",
    "        elif verbose:\n",
    "            print(\"\\nüßº Clean answer:\\n\", final_answer)\n",
    "        else:\n",
    "            return final_answer\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n‚úÖ Response time: {round(time.time() - start, 2)}s\")\n",
    "\n",
    "        if show_history:\n",
    "            print(\"\\nüìú Message History:\")\n",
    "            for msg in chat_memory:\n",
    "                print(f\"{msg['role'].upper()}: {msg['content']}\\n\")\n",
    "\n",
    "        return final_answer if verbose else None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error in ask_local_gpt:\", str(e))\n",
    "        return \"Error occurred.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09672382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Thinking...\n",
      "\n",
      "To solve the problem, we perform a simple addition:\n",
      "\n",
      "1. Start with the first number: **1**.\n",
      "2. Add the second number: **1**.\n",
      "3. Perform the addition: \\(1 + 1 = 2\\).\n",
      "\n",
      "So, **1 + 1 equals 2**.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To solve the problem, we perform a simple addition:\n",
       "\n",
       "1. Start with the first number: **1**.\n",
       "2. Add the second number: **1**.\n",
       "3. Perform the addition: \\(1 + 1 = 2\\).\n",
       "\n",
       "So, **1 + 1 equals 2**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_local_gpt(\"What's 1 + 1?\", stream_response=True, reasoning_mode=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74e5e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_log = pd.DataFrame(conversation_log)\n",
    "df_log.to_csv(\"chat_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ecfc27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prompt</th>\n",
       "      <th>thoughts</th>\n",
       "      <th>answer</th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-06 22:01:54</td>\n",
       "      <td>What's 1 + 1?</td>\n",
       "      <td></td>\n",
       "      <td>To solve the problem, we perform a simple addi...</td>\n",
       "      <td>To solve the problem, we perform a simple addi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp         prompt thoughts  \\\n",
       "0  2025-08-06 22:01:54  What's 1 + 1?            \n",
       "\n",
       "                                              answer  \\\n",
       "0  To solve the problem, we perform a simple addi...   \n",
       "\n",
       "                                                 raw  \n",
       "0  To solve the problem, we perform a simple addi...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2668bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
